<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en-US">
<head>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
<title>Examples - Solaris Dynamic Tracing Guide</title>
<meta name="robots" content="index,follow">
<meta name="robots" content="index,follow">
<meta name="date" content="2008-11-01">
<meta name="collection" content="reference">
<link rel="stylesheet" type="text/css" href="css/elements.css">
<link rel="stylesheet" type="text/css" href="css/indiana.css">
</head>

<body>


<div class="Masthead">
   <div class="MastheadLogo">
      <a href="http://www.opensolaris.org"><img border="0" src="graphics/header.png"></img></a>
   </div>
   <div class="Title">Solaris Dynamic Tracing Guide</div>
</div>

<table class="Layout" border="0" cellspacing="0" width="100%">
<tbody>

   <tr valign="top" class="PageControls">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="chp-sched-1.html">Previous</a>
             </td>
             <td align="right">
                 <a href="chp-sched-stability.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
   
   <tr valign="top">
      <td class="Navigation" width="200px"><p class="toc level1"><a href="docinfo.html">Document Information</a></p>
<p class="toc level1 tocsp"><a href="preface.html">Preface</a></p>
<p class="toc level1 tocsp"><a href="chp-intro.html">1.&nbsp;&nbsp;Introduction</a></p>
<p class="toc level1 tocsp"><a href="chp-typeopexpr.html">2.&nbsp;&nbsp;Types, Operators, and Expressions</a></p>
<p class="toc level1 tocsp"><a href="chp-variables.html">3.&nbsp;&nbsp;Variables</a></p>
<p class="toc level1 tocsp"><a href="chp-prog.html">4.&nbsp;&nbsp;D Program Structure</a></p>
<p class="toc level1 tocsp"><a href="chp-pointers.html">5.&nbsp;&nbsp;Pointers and Arrays</a></p>
<p class="toc level1 tocsp"><a href="chp-strings.html">6.&nbsp;&nbsp;Strings</a></p>
<p class="toc level1 tocsp"><a href="chp-structs.html">7.&nbsp;&nbsp;Structs and Unions</a></p>
<p class="toc level1 tocsp"><a href="chp-types.html">8.&nbsp;&nbsp;Type and Constant Definitions</a></p>
<p class="toc level1 tocsp"><a href="chp-aggs.html">9.&nbsp;&nbsp;Aggregations</a></p>
<p class="toc level1 tocsp"><a href="chp-actsub.html">10.&nbsp;&nbsp;Actions and Subroutines</a></p>
<p class="toc level1 tocsp"><a href="chp-buf.html">11.&nbsp;&nbsp;Buffers and Buffering</a></p>
<p class="toc level1 tocsp"><a href="chp-fmt.html">12.&nbsp;&nbsp;Output Formatting</a></p>
<p class="toc level1 tocsp"><a href="chp-spec.html">13.&nbsp;&nbsp;Speculative Tracing</a></p>
<p class="toc level1 tocsp"><a href="chp-dtrace1m.html">14.&nbsp;&nbsp;<tt>dtrace</tt>(1M) Utility</a></p>
<p class="toc level1 tocsp"><a href="chp-script.html">15.&nbsp;&nbsp;Scripting</a></p>
<p class="toc level1 tocsp"><a href="chp-opt.html">16.&nbsp;&nbsp;Options and Tunables</a></p>
<p class="toc level1 tocsp"><a href="chp-dtrace.html">17.&nbsp;&nbsp;<tt>dtrace</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-lockstat.html">18.&nbsp;&nbsp;<tt>lockstat</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-profile.html">19.&nbsp;&nbsp;<tt>profile</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-fbt.html">20.&nbsp;&nbsp;<tt>fbt</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-syscall.html">21.&nbsp;&nbsp;<tt>syscall</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-sdt.html">22.&nbsp;&nbsp;<tt>sdt</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-sysinfo.html">23.&nbsp;&nbsp;<tt>sysinfo</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-vminfo.html">24.&nbsp;&nbsp;<tt>vminfo</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-proc.html">25.&nbsp;&nbsp;<tt>proc</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-sched.html">26.&nbsp;&nbsp;<tt>sched</tt> Provider</a></p>
<p class="toc level2"><a href="gelro.html">Probes</a></p>
<p class="toc level2"><a href="chp-sched-1.html">Arguments</a></p>
<div class="onpage">
<p class="toc level2"><a href="">Examples</a></p>
</div>
<p class="toc level2"><a href="chp-sched-stability.html">Stability</a></p>
<p class="toc level1 tocsp"><a href="chp-io.html">27.&nbsp;&nbsp;<tt>io</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-mib.html">28.&nbsp;&nbsp;<tt>mib</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-fpuinfo.html">29.&nbsp;&nbsp;<tt>fpuinfo</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-pid.html">30.&nbsp;&nbsp;<tt>pid</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-plockstat.html">31.&nbsp;&nbsp;<tt>plockstat</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-fasttrap.html">32.&nbsp;&nbsp;<tt>fasttrap</tt> Provider</a></p>
<p class="toc level1 tocsp"><a href="chp-user.html">33.&nbsp;&nbsp;User Process Tracing</a></p>
<p class="toc level1 tocsp"><a href="chp-usdt.html">34.&nbsp;&nbsp;Statically Defined Tracing for User Applications</a></p>
<p class="toc level1 tocsp"><a href="chp-sec.html">35.&nbsp;&nbsp;Security</a></p>
<p class="toc level1 tocsp"><a href="chp-anon.html">36.&nbsp;&nbsp;Anonymous Tracing</a></p>
<p class="toc level1 tocsp"><a href="chp-post.html">37.&nbsp;&nbsp;Postmortem Tracing</a></p>
<p class="toc level1 tocsp"><a href="chp-perf.html">38.&nbsp;&nbsp;Performance Considerations</a></p>
<p class="toc level1 tocsp"><a href="chp-stab.html">39.&nbsp;&nbsp;Stability</a></p>
<p class="toc level1 tocsp"><a href="chp-xlate.html">40.&nbsp;&nbsp;Translators</a></p>
<p class="toc level1 tocsp"><a href="chp-vers.html">41.&nbsp;&nbsp;Versioning</a></p>
<p class="toc level1 tocsp"><a href="gloss01.html">Glossary</a></p>
<p class="toc level1 tocsp"><a href="idx-1.html">Index</a></p>
</td>
      <td class="ContentPane" width="705px">

	 <div class="MainContent">      	 
             

<a name="chp-sched-2"></a><h3>Examples</h3>


<a name="chp-sched-5"></a><h4><tt>on-cpu</tt> and <tt>off-cpu</tt></h4>
<p>One common question you might want answered is which CPUs are running threads
and for how long. You can use the <tt>on-cpu</tt> and <tt>off-cpu</tt> probes
to easily answer this question on a system-wide basis as shown in the
following example:</p><pre>sched:::on-cpu
{
    self-&gt;ts = timestamp;
}

sched:::off-cpu
/self-&gt;ts/
{
    @[cpu] = quantize(timestamp - self-&gt;ts);
    self-&gt;ts = 0;
}</pre><p>Running the above script results in output similar to the following example:</p><pre><tt><b># dtrace -s ./where.d</b></tt>
dtrace: script './where.d' matched 5 probes
<tt><b>^C</b></tt>

        0
           value  ------------- Distribution ------------- count    
            2048 |                                         0        
            4096 |@@                                       37       
            8192 |@@@@@@@@@@@@@                            212      
           16384 |@                                        30       
           32768 |                                         10       
           65536 |@                                        17       
          131072 |                                         12       
          262144 |                                         9        
          524288 |                                         6        
         1048576 |                                         5        
         2097152 |                                         1        
         4194304 |                                         3        
         8388608 |@@@@                                     75       
        16777216 |@@@@@@@@@@@@                             201      
        33554432 |                                         6        
        67108864 |                                         0        

        1
           value  ------------- Distribution ------------- count    
            2048 |                                         0        
            4096 |@                                        6        
            8192 |@@@@                                     23       
           16384 |@@@                                      18       
           32768 |@@@@                                     22       
           65536 |@@@@                                     22       
          131072 |@                                        7        
          262144 |                                         5        
          524288 |                                         2        
         1048576 |                                         3        
         2097152 |@                                        9        
         4194304 |                                         4        
         8388608 |@@@                                      18       
        16777216 |@@@                                      19       
        33554432 |@@@                                      16       
        67108864 |@@@@                                     21       
       134217728 |@@                                       14       
       268435456 |                                         0</pre><p>The above output shows that on CPU 1 threads tend to run
for less than 100 microseconds at a stretch, or for approximately 10 milliseconds.
A noticeable gap between the two clusters of data shown in the histogram.
You also might be interested in knowing which CPUs are running a particular
process. You can use the <tt>on-cpu</tt> and <tt>off-cpu</tt> probes for answering this question as
well. The following script displays which CPUs run a specified application over a
period of ten seconds:</p><pre>#pragma D option quiet

dtrace:::BEGIN
{
    start = timestamp;
}

sched:::on-cpu
/execname == $$1/
{
    self-&gt;ts = timestamp;
}

sched:::off-cpu
/self-&gt;ts/
{
    @[cpu] = sum(timestamp - self-&gt;ts);
    self-&gt;ts = 0;
}

profile:::tick-1sec
/++x == 10/
{
    exit(0);
}
        
dtrace:::END
{
    printf("CPU distribution of imapd over %d seconds:\n\n",
        (timestamp - start) / 1000000000);
    printf("CPU microseconds\n--- ------------\n");
    normalize(@, 1000);
    printa("%3d %@d\n", @);
}</pre><p>Running the above script on a large mail server and specifying the
IMAP daemon results in output similar to the following example:</p><pre><tt><b># dtrace -s ./whererun.d imapd</b></tt>
CPU distribution of imapd over 10 seconds:

CPU microseconds
--- ------------
 15 10102
 12 16377
 21 25317
 19 25504
 17 35653
 13 41539
 14 46669
 20 57753
 22 70088
 16 115860
 23 127775
 18 160517</pre><p>Solaris takes into account the amount of time that a thread has
been sleeping when selecting a CPU on which to run the thread: a
thread that has been sleeping for less time tends not to migrate. You
can use the <tt>off-cpu</tt> and <tt>on-cpu</tt> probes to observe this behavior:</p><pre>sched:::off-cpu
/curlwpsinfo-&gt;pr_state == SSLEEP/
{
    self-&gt;cpu = cpu;
    self-&gt;ts = timestamp;
}

sched:::on-cpu
/self-&gt;ts/
{
    @[self-&gt;cpu == cpu ?
        "sleep time, no CPU migration" : "sleep time, CPU migration"] =
        lquantize((timestamp - self-&gt;ts) / 1000000, 0, 500, 25);
    self-&gt;ts = 0;
    self-&gt;cpu = 0;
}</pre><p>Running the above script for approximately 30 seconds results in output similar to
the following example:</p><pre><tt><b># dtrace -s ./howlong.d</b></tt>
dtrace: script './howlong.d' matched 5 probes
<tt><b>^C</b></tt>
 sleep time, CPU migration                         
           value  -------------- Distribution ------------ count    
             &lt; 0 |                                         0        
               0 |@@@@@@@                                  6838     
              25 |@@@@@                                    4714     
              50 |@@@                                      3108     
              75 |@                                        1304     
             100 |@                                        1557     
             125 |@                                        1425     
             150 |                                         894      
             175 |@                                        1526     
             200 |@@                                       2010     
             225 |@@                                       1933     
             250 |@@                                       1982     
             275 |@@                                       2051     
             300 |@@                                       2021     
             325 |@                                        1708     
             350 |@                                        1113     
             375 |                                         502      
             400 |                                         220      
             425 |                                         106      
             450 |                                         54       
             475 |                                         40       
          &gt;= 500 |@                                        1716     

  sleep time, no CPU migration                      
           value  -------------- Distribution ------------ count    
             &lt; 0 |                                         0        
               0 |@@@@@@@@@@@@                             58413    
              25 |@@@                                      14793    
              50 |@@                                       10050    
              75 |                                         3858     
             100 |@                                        6242     
             125 |@                                        6555     
             150 |                                         3980     
             175 |@                                        5987     
             200 |@                                        9024     
             225 |@                                        9070     
             250 |@@                                       10745    
             275 |@@                                       11898    
             300 |@@                                       11704    
             325 |@@                                       10846    
             350 |@                                        6962     
             375 |                                         3292     
             400 |                                         1713     
             425 |                                         585      
             450 |                                         201      
             475 |                                         96       
          &gt;= 500 |                                         3946</pre><p>The example output shows that there are many more occurences of non-migration than
migration. Also, when sleep times are longer, migrations are more likely. The distributions
are noticeably different in the sub-100 millisecond range, but look very similar as
the sleep times get longer. This result would seem to indicate that sleep
time is not factored into the scheduling decision once a certain threshold is
exceeded.</p><p>The final example using <tt>off-cpu</tt> and <tt>on-cpu</tt> shows how to use these probes
along with the <tt>pr_stype</tt> field to determine why threads sleep and for how
long:</p><pre>sched:::off-cpu
/curlwpsinfo-&gt;pr_state == SSLEEP/
{
    /*
     * We're sleeping.  Track our sobj type.
     */
    self-&gt;sobj = curlwpsinfo-&gt;pr_stype;
    self-&gt;bedtime = timestamp;
}

sched:::off-cpu
/curlwpsinfo-&gt;pr_state == SRUN/
{
    self-&gt;bedtime = timestamp;
}

sched:::on-cpu
/self-&gt;bedtime &amp;&amp; !self-&gt;sobj/
{
    @["preempted"] = quantize(timestamp - self-&gt;bedtime);
    self-&gt;bedtime = 0;
}

sched:::on-cpu
/self-&gt;sobj/
{
    @[self-&gt;sobj == SOBJ_MUTEX ? "kernel-level lock" :
        self-&gt;sobj == SOBJ_RWLOCK ? "rwlock" :
        self-&gt;sobj == SOBJ_CV ? "condition variable" :
        self-&gt;sobj == SOBJ_SEMA ? "semaphore" :
        self-&gt;sobj == SOBJ_USER ? "user-level lock" :
        self-&gt;sobj == SOBJ_USER_PI ? "user-level prio-inheriting lock" :
        self-&gt;sobj == SOBJ_SHUTTLE ? "shuttle" : "unknown"] =
        quantize(timestamp - self-&gt;bedtime);

    self-&gt;sobj = 0;
    self-&gt;bedtime = 0;
}</pre><p>Running the above script for several seconds results in output similar to the
following example:</p><pre><tt><b># dtrace -s ./whatfor.d</b></tt>
dtrace: script './whatfor.d' matched 12 probes
<tt><b>^C</b></tt>
 kernel-level lock
           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |@@@@@@@@                                 3        
           65536 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@            11       
          131072 |@@                                       1        
          262144 |                                         0        

  preempted
           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |                                         4        
           65536 |@@@@@@@@                                 408      
          131072 |@@@@@@@@@@@@@@@@@@@@@@                   1031     
          262144 |@@@                                      156      
          524288 |@@                                       116      
         1048576 |@                                        51       
         2097152 |                                         42       
         4194304 |                                         16       
         8388608 |                                         15       
        16777216 |                                         4        
        33554432 |                                         8        
        67108864 |                                         0        

  semaphore
           value  -------------- Distribution ------------ count    
           32768 |                                         0        
           65536 |@@                                       61       
          131072 |@@@@@@@@@@@@@@@@@@@@@@@@                 553      
          262144 |@@                                       63       
          524288 |@                                        36       
         1048576 |                                         7        
         2097152 |                                         22       
         4194304 |@                                        44       
         8388608 |@@@                                      84       
        16777216 |@                                        36       
        33554432 |                                         3        
        67108864 |                                         6        
       134217728 |                                         0        
       268435456 |                                         0        
       536870912 |                                         0        
      1073741824 |                                         0        
      2147483648 |                                         0        
      4294967296 |                                         0        
      8589934592 |                                         0        
     17179869184 |                                         1        
     34359738368 |                                         0        

  shuttle                                           
           value  -------------- Distribution ------------ count    
           32768 |                                         0        
           65536 |@@@@@                                    2        
          131072 |@@@@@@@@@@@@@@@@                         6        
          262144 |@@@@@                                    2        
          524288 |                                         0        
         1048576 |                                         0        
         2097152 |                                         0        
         4194304 |@@@@@                                    2        
         8388608 |                                         0        
        16777216 |                                         0        
        33554432 |                                         0        
        67108864 |                                         0        
       134217728 |                                         0        
       268435456 |                                         0        
       536870912 |                                         0        
      1073741824 |                                         0        
      2147483648 |                                         0        
      4294967296 |@@@@@                                    2        
      8589934592 |                                         0        
     17179869184 |@@                                       1        
     34359738368 |                                         0        

  condition variable                                
           value  -------------- Distribution ------------ count    
           32768 |                                         0        
           65536 |                                         122      
          131072 |@@@@@                                    1579     
          262144 |@                                        340      
          524288 |                                         268      
         1048576 |@@@                                      1028     
         2097152 |@@@                                      1007     
         4194304 |@@@                                      1176     
         8388608 |@@@@                                     1257     
        16777216 |@@@@@@@@@@@@@@                           4385     
        33554432 |                                         295      
        67108864 |                                         157      
       134217728 |                                         96       
       268435456 |                                         48       
       536870912 |                                         144      
      1073741824 |                                         10       
      2147483648 |                                         22       
      4294967296 |                                         18       
      8589934592 |                                         5        
     17179869184 |                                         6        
     34359738368 |                                         4        
     68719476736 |                                         0</pre>

<a name="chp-sched-10"></a><h4><tt>enqueue</tt> and <tt>dequeue</tt></h4>
<p>When a CPU becomes idle, the dispatcher looks for work enqueued on
other (non-idle) CPUs. The following example uses the <tt>dequeue</tt> probe to understand how often
applications are transferred and by which CPU:</p><pre>#pragma D option quiet

sched:::dequeue
/args[2]-&gt;cpu_id != --1 &amp;&amp; cpu != args[2]-&gt;cpu_id &amp;&amp;
    (curlwpsinfo-&gt;pr_flag &amp; PR_IDLE)/
{
    @[stringof(args[1]-&gt;pr_fname), args[2]-&gt;cpu_id] =
        lquantize(cpu, 0, 100);
}
END
{
    printa("%s stolen from CPU %d by:\n%@d\n", @);
}</pre><p>The tail of the output from running the above script on a
4 CPU system results in output similar to the following example:</p><pre><tt><b># dtrace -s ./whosteal.d</b></tt>
<tt><b>^C</b></tt>
...
 nscd stolen from CPU 1 by:

           value  -------------- Distribution ------------ count    
               1 |                                         0        
               2 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 28       
               3 |                                         0        

snmpd stolen from CPU 1 by:

           value  -------------- Distribution ------------ count    
             &lt; 0 |                                         0        
               0 |@                                        1        
               1 |                                         0        
               2 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@     31       
               3 |@@                                       2        
               4 |                                         0        

sched stolen from CPU 1 by:

           value  -------------- Distribution ------------ count    
             &lt; 0 |                                         0        
               0 |@@                                       3        
               1 |                                         0        
               2 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@         36       
               3 |@@@@                                     5        
               4 |                                         0</pre><p>Instead of knowing which CPUs took which work, you might want to
know the CPUs on which processes and threads are waiting to run. You
can use the <tt>enqueue</tt> and <tt>dequeue</tt> probes together to answer this question:</p><pre>sched:::enqueue
{
    self-&gt;ts = timestamp;
}

sched:::dequeue
/self-&gt;ts/
{
    @[args[2]-&gt;cpu_id] = quantize(timestamp - self-&gt;ts);
    self-&gt;ts = 0;
}</pre><p>Running the above script for several seconds results in output similar to the
following example:</p><pre><tt><b># dtrace -s ./qtime.d</b></tt>
dtrace: script './qtime.d' matched 5 probes
<tt><b>^C</b></tt>
       -1
           value  -------------- Distribution ------------ count    
            4096 |                                         0        
            8192 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 2        
           16384 |                                         0        

        0
           value  -------------- Distribution ------------ count    
            1024 |                                         0        
            2048 |@@@@@@@@@@@@@@@                          262      
            4096 |@@@@@@@@@@@@@                            227      
            8192 |@@@@@                                    87       
           16384 |@@@                                      54       
           32768 |                                         7        
           65536 |                                         9        
          131072 |                                         1        
          262144 |                                         5        
          524288 |                                         4        
         1048576 |                                         2        
         2097152 |                                         0        
         4194304 |                                         0        
         8388608 |                                         0        
        16777216 |                                         1        
        33554432 |                                         2        
        67108864 |                                         2        
       134217728 |                                         0        
       268435456 |                                         0        
       536870912 |                                         0        
      1073741824 |                                         1        
      2147483648 |                                         1        
      4294967296 |                                         0        

        1
           value  -------------- Distribution ------------ count    
            1024 |                                         0        
            2048 |@@@@                                     49       
            4096 |@@@@@@@@@@@@@@@@@@@@                     241      
            8192 |@@@@@@@                                  91       
           16384 |@@@@                                     55       
           32768 |                                         7        
           65536 |                                         3        
          131072 |                                         2        
          262144 |                                         1        
          524288 |                                         0        
         1048576 |                                         0        
         2097152 |                                         0        
         4194304 |                                         0        
         8388608 |                                         0        
        16777216 |                                         0        
        33554432 |                                         3        
        67108864 |                                         1        
       134217728 |                                         4        
       268435456 |                                         2        
       536870912 |                                         0        
      1073741824 |                                         3        
      2147483648 |                                         2        
      4294967296 |                                         0</pre><p>Notice the non-zero values at the bottom of the example output. These data
points reveal several instances on both CPUs where a thread was enqueued to
run for several <b>seconds</b>.</p><p>Instead of looking at wait times, you might want to examine the
length of the run queue over time. Using the <tt>enqueue</tt> and <tt>dequeue</tt> probes,
you can set up an associative array to track the queue length:</p><pre>sched:::enqueue
{
    this-&gt;len = qlen[args[2]-&gt;cpu_id]++;
    @[args[2]-&gt;cpu_id] = lquantize(this-&gt;len, 0, 100);
}

sched:::dequeue
/qlen[args[2]-&gt;cpu_id]/
{
    qlen[args[2]-&gt;cpu_id]&mdash;;
}</pre><p>Running the above script for approximately 30 seconds on a largely idle uniprocessor
laptop system results in output similar to the following example:</p><pre><tt><b># dtrace -s ./qlen.d</b></tt>
dtrace: script './qlen.d' matched 5 probes
<tt><b>^C</b></tt>
        0
           value  -------------- Distribution ------------ count    
             &lt; 0 |                                         0        
               0 |@@@@@@@@@@@@@@@@@@@@@@@@@                110626   
               1 |@@@@@@@@@                                41142    
               2 |@@                                       12655    
               3 |@                                        5074     
               4 |                                         1722     
               5 |                                         701      
               6 |                                         302      
               7 |                                         63       
               8 |                                         23       
               9 |                                         12       
              10 |                                         24       
              11 |                                         58       
              12 |                                         14       
              13 |                                         3        
              14 |                                         0</pre><p>The output is roughly what you would expect for an idle system:
the majority of the time that a runnable thread is enqueued, the run
queue was very short (three or fewer threads in length). However, given that
the system was largely idle, the exceptional data points at the bottom of
the table might be unexpected. For example, why was the run queue as
long as 13 runnable threads? To explore this question, you could write a
D script that displays the contents of the run queue when the length
of the run queue is long. This problem is complicated because D enablings
cannot iterate over data structures, and therefore cannot simply iterate over the entire
run queue. Even if D enablings could do so, you should avoid dependencies
on the kernel's internal data structures.</p><p>For this type of script, you would enable the <tt>enqueue</tt> and <tt>dequeue</tt>
probes and use both speculations and associative arrays. Whenever a thread is enqueued,
the script increments the length of the queue and records the timestamp in
an associative array keyed by the thread. You cannot use a thread-local variable
in this case because a thread might be enqueued by another thread. The
script then checks to see if the queue length exceeds the maximum. If
it does, the script starts a new speculation, and records the timestamp and
the new maximum. Then, when a thread is dequeued, the script compares the
enqueue timestamp to the timestamp of the longest length: if the thread was
enqueued <b>before</b> the timestamp of the longest length, the thread was in the
queue when the longest length was recorded. In this case, the script speculatively
traces the thread's information. Once the kernel dequeues the last thread that was
enqueued at the timestamp of the longest length, the script commits the speculation
data. This script is shown below:</p><pre>#pragma D option quiet
#pragma D option nspec=4
#pragma D option specsize=100k

int maxlen;
int spec[int];

sched:::enqueue
{
    this-&gt;len = ++qlen[this-&gt;cpu = args[2]-&gt;cpu_id];
    in[args[0]-&gt;pr_addr] = timestamp;
}

sched:::enqueue
/this-&gt;len &gt; maxlen &amp;&amp; spec[this-&gt;cpu]/
{
    /*
     * There is already a speculation for this CPU.  We just set a new
     * record, so we'll discard the old one.
     */
    discard(spec[this-&gt;cpu]);
}

sched:::enqueue
/this-&gt;len &gt; maxlen/
{
    /*
     * We have a winner.  Set the new maximum length and set the timestamp
     * of the longest length.
     */
    maxlen = this-&gt;len;
    longtime[this-&gt;cpu] = timestamp;    

    /*
     * Now start a new speculation, and speculatively trace the length.
     */
    this-&gt;spec = spec[this-&gt;cpu] = speculation();
    speculate(this-&gt;spec);
    printf("Run queue of length %d:\n", this-&gt;len);
}

sched:::dequeue
/(this-&gt;in = in[args[0]-&gt;pr_addr]) &amp;&amp;
    this-&gt;in &lt;= longtime[this-&gt;cpu = args[2]-&gt;cpu_id]/
{
    speculate(spec[this-&gt;cpu]);
    printf("  %d/%d (%s)\n", 
        args[1]-&gt;pr_pid, args[0]-&gt;pr_lwpid,
        stringof(args[1]-&gt;pr_fname));
}

sched:::dequeue
/qlen[args[2]-&gt;cpu_id]/
{
    in[args[0]-&gt;pr_addr] = 0;
    this-&gt;len = --qlen[args[2]-&gt;cpu_id];
}

sched:::dequeue
/this-&gt;len == 0 &amp;&amp; spec[this-&gt;cpu]/
{
    /*
     * We just processed the last thread that was enqueued at the time
     * of longest length; commit the speculation, which by now contains
     * each thread that was enqueued when the queue was longest.
     */
    commit(spec[this-&gt;cpu]);
    spec[this-&gt;cpu] = 0;
}</pre><p>Running the above script on the same uniprocessor laptop results in output similar
to the following example:</p><pre><tt><b># dtrace -s ./whoqueue.d</b></tt>
Run queue of length 3:
 0/0 (sched)
  0/0 (sched)
  101170/1 (dtrace)
Run queue of length 4:
  0/0 (sched)
  100356/1 (Xsun)
  100420/1 (xterm)
  101170/1 (dtrace)
Run queue of length 5:
  0/0 (sched)
  0/0 (sched)
  100356/1 (Xsun)
  100420/1 (xterm)
  101170/1 (dtrace)
Run queue of length 7:
  0/0 (sched)
  100221/18 (nscd)
  100221/17 (nscd)
  100221/16 (nscd)
  100221/13 (nscd)
  100221/14 (nscd)
  100221/15 (nscd)
Run queue of length 16:
  100821/1 (xterm)
  100768/1 (xterm)
  100365/1 (fvwm2)
  101118/1 (xterm)
  100577/1 (xterm)
  101170/1 (dtrace)
  101020/1 (xterm)
  101089/1 (xterm)
  100795/1 (xterm)
  100741/1 (xterm)
  100710/1 (xterm)
  101048/1 (xterm)
  100697/1 (MozillaFirebird-)
  100420/1 (xterm)
  100394/1 (xterm)
  100368/1 (xterm)
<tt><b>^C</b></tt></pre><p>The output reveals that the long run queues are due to many
runnable <tt>xterm</tt> processes. This experiment coincided with a change in virtual desktop, and
therefore the results are probably due to some sort of X event processing.</p>

<a name="chp-sched-11"></a><h4><tt>sleep</tt> and <tt>wakeup</tt></h4>
<p>In <a href="#chp-sched-10"><tt>enqueue</tt> and <tt>dequeue</tt></a>, the final example demonstrated that a burst in run queue length
was due to runnable <tt>xterm</tt> processes. One hypothesis is that the observations
resulted from a change in virtual desktop. You can use the <tt>wakeup</tt> probe to
explore this hypothesis by determining who is waking the <tt>xterm</tt> processes, and when,
as shown in the following example:</p><pre>#pragma D option quiet

dtrace:::BEGIN
{
    start = timestamp;
}

sched:::wakeup
/stringof(args[1]-&gt;pr_fname) == "xterm"/
{
    @[execname] = lquantize((timestamp - start) / 1000000000, 0, 10);
}

profile:::tick-1sec
/++x == 10/
{
    exit(0);
}</pre><p>To investigate the hypothesis, run the above script, waiting roughly five seconds, and
switch your virtual desktop exactly once. If the burst of runnable <tt>xterm</tt> processes
is due to switching the virtual desktop, the output should show a burst
of wakeup activity at the five second mark.</p><pre><tt><b># dtrace -s ./xterm.d</b></tt>

  Xsun

           value  -------------- Distribution ------------ count    
               4 |                                         0        
               5 |@                                        1        
               6 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   32       
               7 |                                         0</pre><p>The output does show that the X server is waking xterm processes,
clustered around the time that you switched virtual desktops. If you wanted to
understand the interaction between the X server and the <tt>xterm</tt> processes, you could aggregate
on user stack traces when the X server fires the <tt>wakeup</tt> probe.</p><p>Understanding the performance of client/server systems like the X windowing system requires understanding
the clients on whose behalf the server is doing work. This kind of
question is difficult to answer with conventional performance analysis tools. However, if you
have a model where a client sends a message to the server and
sleeps pending the server's processing, you can use the <tt>wakeup</tt> probe to
determine the client for whom the request is being performed, as shown in
the following example:</p><pre>self int last;

sched:::wakeup
/self-&gt;last &amp;&amp; args[0]-&gt;pr_stype == SOBJ_CV/
{
    @[stringof(args[1]-&gt;pr_fname)] = sum(vtimestamp - self-&gt;last);
    self-&gt;last = 0;
}

sched:::wakeup
/execname == "Xsun" &amp;&amp; self-&gt;last == 0/
{
    self-&gt;last = vtimestamp;
}</pre><p>Running the above script results in output similar to the following example:</p><pre><tt><b>dtrace -s ./xwork.d</b></tt>
dtrace: script './xwork.d' matched 14 probes
<tt><b>^C</b></tt>
  xterm                                                       9522510
  soffice.bin                                                 9912594
  fvwm2                                                     100423123
  MozillaFirebird                                           312227077
  acroread                                                  345901577</pre><p>This output reveals that much <tt>Xsun</tt> work is being done on behalf of
the processes <tt>acroread</tt>, <tt>MozillaFirebird</tt> and, to a lesser degree, <tt>fvwm2</tt>. Notice that the
script only examined wakeups from condition variable synchronization objects (<tt>SOBJ_CV</tt>). As described in
<a href="gelse.html#tbl-sched-sobj">Table&nbsp;25-4</a>, condition variables are the type of synchronization object typically used to synchronize
for reasons other than access to a shared data region. In the case
of the X server, a client will wait for data in a
pipe by sleeping on a condition variable.</p><p>You can additionally use the <tt>sleep</tt> probe along with the <tt>wakeup</tt> probe to
understand which applications are blocking on which applications, and for how long, as
shown in the following example:</p><pre>#pragma D option quiet

sched:::sleep
/!(curlwpsinfo-&gt;pr_flag &amp; PR_ISSYS) &amp;&amp; curlwpsinfo-&gt;pr_stype == SOBJ_CV/
{
    bedtime[curlwpsinfo-&gt;pr_addr] = timestamp;
}

sched:::wakeup
/bedtime[args[0]-&gt;pr_addr]/
{
    @[stringof(args[1]-&gt;pr_fname), execname] =
        quantize(timestamp - bedtime[args[0]-&gt;pr_addr]);
    bedtime[args[0]-&gt;pr_addr] = 0;
}

END
{
    printa("%s sleeping on %s:\n%@d\n", @);
}</pre><p>The tail of the output from running the example script for several
seconds on a desktop system resembles the following example:</p><pre><tt><b># dtrace -s ./whofor.d</b></tt>
<tt><b>^C</b></tt>
...
 xterm sleeping on Xsun:

           value  -------------- Distribution ------------ count    
          131072 |                                         0        
          262144 |                                         12       
          524288 |                                         2        
         1048576 |                                         0        
         2097152 |                                         5        
         4194304 |@@@                                      45       
         8388608 |                                         1        
        16777216 |                                         9        
        33554432 |@@@@@                                    83       
        67108864 |@@@@@@@@@@@                              164      
       134217728 |@@@@@@@@@@                               147      
       268435456 |@@@@                                     56       
       536870912 |@                                        17       
      1073741824 |                                         9        
      2147483648 |                                         1        
      4294967296 |                                         3        
      8589934592 |                                         1        
     17179869184 |                                         0        

fvwm2 sleeping on Xsun:

           value  -------------- Distribution ------------ count    
           32768 |                                         0        
           65536 |@@@@@@@@@@@@@@@@@@@@@@                   67       
          131072 |@@@@@                                    16       
          262144 |@@                                       6        
          524288 |@                                        3        
         1048576 |@@@@@                                    15       
         2097152 |                                         0        
         4194304 |                                         0        
         8388608 |                                         1        
        16777216 |                                         0        
        33554432 |                                         0        
        67108864 |                                         1        
       134217728 |                                         0        
       268435456 |                                         0        
       536870912 |                                         1        
      1073741824 |                                         1        
      2147483648 |                                         2        
      4294967296 |                                         2        
      8589934592 |                                         2        
     17179869184 |                                         0        
     34359738368 |                                         2        
     68719476736 |                                         0        

syslogd sleeping on syslogd:

           value  -------------- Distribution ------------ count    
     17179869184 |                                         0        
     34359738368 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 3        
     68719476736 |                                         0        

MozillaFirebird sleeping on MozillaFirebird:

           value  -------------- Distribution ------------ count    
           65536 |                                         0        
          131072 |                                         3        
          262144 |@@                                       14       
          524288 |                                         0        
         1048576 |@@@                                      18       
         2097152 |                                         0        
         4194304 |                                         0        
         8388608 |                                         1        
        16777216 |                                         0        
        33554432 |                                         1        
        67108864 |                                         3        
       134217728 |@                                        7        
       268435456 |@@@@@@@@@@                               53       
       536870912 |@@@@@@@@@@@@@@                           78       
      1073741824 |@@@@                                     25       
      2147483648 |                                         0        
      4294967296 |                                         0        
      8589934592 |@                                        7        
     17179869184 |                                         0</pre><p>You might want to understand how and why <tt>MozillaFirebird</tt> is blocking on itself.
You could modify the above script as shown in the following example to
answer this question:</p><pre>#pragma D option quiet

sched:::sleep
/execname == "MozillaFirebird" &amp;&amp; curlwpsinfo-&gt;pr_stype == SOBJ_CV/
{
    bedtime[curlwpsinfo-&gt;pr_addr] = timestamp;
}

sched:::wakeup
/execname == "MozillaFirebird" &amp;&amp; bedtime[args[0]-&gt;pr_addr]/
{
    @[args[1]-&gt;pr_pid, args[0]-&gt;pr_lwpid, pid, curlwpsinfo-&gt;pr_lwpid] = 
        quantize(timestamp - bedtime[args[0]-&gt;pr_addr]);
    bedtime[args[0]-&gt;pr_addr] = 0;
}

sched:::wakeup
/bedtime[args[0]-&gt;pr_addr]/
{
    bedtime[args[0]-&gt;pr_addr] = 0;
}

END
{
    printa("%d/%d sleeping on %d/%d:\n%@d\n", @);
}</pre><p>Running the modified script for several seconds results in output similar to the
following example:</p><pre><tt><b># dtrace -s ./firebird.d</b></tt>
<tt><b>^C</b></tt>

 100459/1 sleeping on 100459/13:

           value  -------------- Distribution ------------ count    
          262144 |                                         0        
          524288 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 1        
         1048576 |                                         0        

100459/13 sleeping on 100459/1:

           value  -------------- Distribution ------------ count    
        16777216 |                                         0        
        33554432 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 1        
        67108864 |                                         0        

100459/1 sleeping on 100459/2:

           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |@@@@                                     5        
           65536 |@                                        2        
          131072 |@@@@@                                    6        
          262144 |                                         1        
          524288 |@                                        2        
         1048576 |                                         0        
         2097152 |@@                                       3        
         4194304 |@@@@                                     5        
         8388608 |@@@@@@@@                                 9        
        16777216 |@@@@@                                    6        
        33554432 |@@                                       3        
        67108864 |                                         0        

100459/1 sleeping on 100459/5:

           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |@@@@@                                    12       
           65536 |@@                                       5        
          131072 |@@@@@@                                   15       
          262144 |                                         1        
          524288 |                                         1        
         1048576 |                                         2        
         2097152 |@                                        4        
         4194304 |@@@@@                                    13       
         8388608 |@@@                                      8        
        16777216 |@@@@@                                    13       
        33554432 |@@                                       6        
        67108864 |@@                                       5        
       134217728 |@                                        4        
       268435456 |                                         0        
       536870912 |                                         1        
      1073741824 |                                         0        

100459/2 sleeping on 100459/1:

           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |@@@@@@@@@@@@@@                           11       
           65536 |                                         0        
          131072 |@@                                       2        
          262144 |                                         0        
          524288 |                                         0        
         1048576 |@@@@                                     3        
         2097152 |@                                        1        
         4194304 |@@                                       2        
         8388608 |@@                                       2        
        16777216 |@                                        1        
        33554432 |@@@@@@                                   5        
        67108864 |                                         0        
       134217728 |                                         0        
       268435456 |                                         0        
       536870912 |@                                        1        
      1073741824 |@                                        1        
      2147483648 |@                                        1        
      4294967296 |                                         0        

100459/5 sleeping on 100459/1:

           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |                                         1        
           65536 |                                         2        
          131072 |                                         4        
          262144 |                                         7        
          524288 |                                         1        
         1048576 |                                         5        
         2097152 |                                         10       
         4194304 |@@@@@@                                   77       
         8388608 |@@@@@@@@@@@@@@@@@@@@@@@                  270      
        16777216 |@@@                                      43       
        33554432 |@                                        20       
        67108864 |@                                        14       
       134217728 |                                         5        
       268435456 |                                         2        
       536870912 |                                         1        
      1073741824 |                                         0        </pre><p>You can also use the <tt>sleep</tt> and <tt>wakeup</tt> probes to understand the performance
of door servers such as the name service cache daemon, as shown in
the following example:</p><pre>sched:::sleep
/curlwpsinfo-&gt;pr_stype == SOBJ_SHUTTLE/
{
    bedtime[curlwpsinfo-&gt;pr_addr] = timestamp;
}

sched:::wakeup
/execname == "nscd" &amp;&amp; bedtime[args[0]-&gt;pr_addr]/
{
    @[stringof(curpsinfo-&gt;pr_fname), stringof(args[1]-&gt;pr_fname)] =
        quantize(timestamp - bedtime[args[0]-&gt;pr_addr]);
    bedtime[args[0]-&gt;pr_addr] = 0;
}

sched:::wakeup
/bedtime[args[0]-&gt;pr_addr]/
{
    bedtime[args[0]-&gt;pr_addr] = 0;
}</pre><p>The tail of the output from running the above script on a
large mail server resembles the following example:</p><pre>imapd
           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |                                         2        
           65536 |@@@@@@@@@@@@@@@@@                        57       
          131072 |@@@@@@@@@@@                              37       
          262144 |                                         3        
          524288 |@@@                                      11       
         1048576 |@@@                                      10       
         2097152 |@@                                       9        
         4194304 |                                         1        
         8388608 |                                         0        

  mountd                                            
           value  -------------- Distribution ------------ count    
           65536 |                                         0        
          131072 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@            49       
          262144 |@@@                                      6        
          524288 |                                         1        
         1048576 |                                         0        
         2097152 |                                         0        
         4194304 |@@@@                                     7        
         8388608 |@                                        3        
        16777216 |                                         0        

  sendmail
           value  -------------- Distribution ------------ count    
           16384 |                                         0        
           32768 |@                                        18       
           65536 |@@@@@@@@@@@@@@@@@                        205      
          131072 |@@@@@@@@@@@@@                            154      
          262144 |@                                        23       
          524288 |                                         5        
         1048576 |@@@@                                     50       
         2097152 |                                         7        
         4194304 |                                         5        
         8388608 |                                         2        
        16777216 |                                         0        

  automountd                                        
           value  -------------- Distribution ------------ count    
           32768 |                                         0        
           65536 |@@@@@@@@@@                               22       
          131072 |@@@@@@@@@@@@@@@@@@@@@@@                  51       
          262144 |@@                                       6        
          524288 |                                         1        
         1048576 |                                         0        
         2097152 |                                         2        
         4194304 |                                         2        
         8388608 |                                         1        
        16777216 |                                         1        
        33554432 |                                         1        
        67108864 |                                         0        
       134217728 |                                         0        
       268435456 |                                         1        
       536870912 |                                         0</pre><p>You might be interested in the unusual data points for <tt>automountd</tt> or the
persistent data point at over one millisecond for <tt>sendmail</tt>. You can add additional
predicates to the above script to hone in on the causes of any
exceptional or anomalous results.</p>

<a name="chp-sched-12"></a><h4><tt>preempt</tt>, <tt>remain-cpu</tt></h4>
<p>Because Solaris is a preemptive system, higher priority threads preempt lower priority ones.
Preemption can induce a significant latency bubble in the lower priority thread, so
you might want to know which threads are being preempted by which other
threads. The following example shows how to use the <tt>preempt</tt> and <tt>remain-cpu</tt>
probes to display this information:</p><pre>#pragma D option quiet

sched:::preempt
{
    self-&gt;preempt = 1;
}

sched:::remain-cpu
/self-&gt;preempt/
{
    self-&gt;preempt = 0;
}

sched:::off-cpu
/self-&gt;preempt/
{
    /*
     * If we were told to preempt ourselves, see who we ended up giving
     * the CPU to.
     */
    @[stringof(args[1]-&gt;pr_fname), args[0]-&gt;pr_pri, execname,
        curlwpsinfo-&gt;pr_pri] = count();
    self-&gt;preempt = 0;
}

END
{
    printf("%30s %3s %30s %3s %5s\n", "PREEMPTOR", "PRI",
        "PREEMPTED", "PRI", "#");
    printa("%30s %3d %30s %3d %5@d\n", @);
}</pre><p>Running the above script for several seconds on a desktop system results in
output similar to the following example:</p><pre><tt><b># dtrace -s ./whopreempt.d</b></tt>
<tt><b>^C</b></tt>
                     PREEMPTOR PRI                      PREEMPTED PRI     #
                         sched  60                           Xsun  53     1
                         xterm  59                           Xsun  53     1
               MozillaFirebird  57                           Xsun  53     1
                        mpstat 100                          fvwm2  59     1
                         sched  99                MozillaFirebird  57     1
                         sched  60                         dtrace  30     1
                        mpstat 100                           Xsun  59     2
                         sched  60                           Xsun  54     2
                         sched  99                          sched  60     2
                         fvwm2  59                           Xsun  44     2
                         sched  99                           Xsun  44     2
                         sched  60                          xterm  59     2
                         sched  99                           Xsun  53     2
                         sched  99                           Xsun  54     3
                         sched  60                          fvwm2  59     3
                         sched  60                           Xsun  59     3
                         sched  99                           Xsun  59     4
                         fvwm2  59                           Xsun  54     8
                         fvwm2  59                           Xsun  53     9
                          Xsun  59                MozillaFirebird  57    10
                         sched  60                MozillaFirebird  57    14
               MozillaFirebird  57                           Xsun  44    16
               MozillaFirebird  57                           Xsun  54    18</pre>

<a name="chp-sched-13"></a><h4><tt>change-pri</tt></h4>
<p>Preemption is based on priorities, so you might want to observe changes in
priority over time. The following example uses the <tt>change-pri</tt> probe to display this
information:</p><pre>sched:::change-pri
{
    @[stringof(args[0]-&gt;pr_clname)] =
        lquantize(args[2] - args[0]-&gt;pr_pri, -50, 50, 5);
}</pre><p>The example script captures the degree to which priority is raised or lowered,
and aggregates by scheduling class. Running the above script results in output similar
to the following example:</p><pre><tt><b># dtrace -s ./pri.d</b></tt>
dtrace: script './pri.d' matched 10 probes
<tt><b>^C</b></tt>
 IA                                                
           value  -------------- Distribution ------------ count    
           &lt; -50 |                                         20       
             -50 |@                                        38       
             -45 |                                         4        
             -40 |                                         13       
             -35 |                                         12       
             -30 |                                         18       
             -25 |                                         18       
             -20 |                                         23       
             -15 |                                         6        
             -10 |@@@@@@@@                                 201      
              -5 |@@@@@@                                   160      
               0 |@@@@@                                    138      
               5 |@                                        47       
              10 |@@                                       66       
              15 |@                                        36       
              20 |@                                        26       
              25 |@                                        28       
              30 |                                         18       
              35 |                                         22       
              40 |                                         8        
              45 |                                         11       
           &gt;= 50 |@                                        34       

  TS                                                
           value  -------------- Distribution ------------ count    
             -15 |                                         0        
             -10 |@                                        1        
              -5 |@@@@@@@@@@@@                             7        
               0 |@@@@@@@@@@@@@@@@@@@@                     12       
               5 |                                         0        
              10 |@@@@@                                    3        
              15 |                                         0</pre><p>The output shows the priority manipulation of the Interactive (IA) scheduling class. Instead
of seeing priority <b>manipulation</b>, you might want to see the priority <b>values</b> of
a particular process and thread over time. The following script uses the <tt>change-pri</tt>
probe to display this information:</p><pre>#pragma D option quiet

BEGIN
{
    start = timestamp;
}

sched:::change-pri
/args[1]-&gt;pr_pid == $1 &amp;&amp; args[0]-&gt;pr_lwpid == $2/
{
    printf("%d %d\n", timestamp - start, args[2]);
}

tick-1sec
/++n == 5/
{
    exit(0);
}</pre><p>To see the change in priorities over time, type the following command in
one window:</p><pre><tt><b>$ echo $$</b></tt>
139208
<tt><b>$ while true ; do let i=0 ; done</b></tt></pre><p>In another window, run the script and redirect the output to a
file:</p><pre><tt><b># dtrace -s ./pritime.d 139208 1 &gt; /tmp/pritime.out</b></tt>
#</pre><p>You can use the file <tt>/tmp/pritime.out</tt> that is generated above as input to
plotting software to graphically display priority over time. <tt>gnuplot</tt> is a freely
available plotting package that is included in the Solaris Freeware Companion CD. By
default, <tt>gnuplot</tt> is installed in <tt>/opt/sfw/bin</tt>.</p>

<a name="chp-sched-15"></a><h4><tt>tick</tt></h4>
<p>Solaris uses <b>tick-based CPU accounting</b>, in which a system clock interrupt fires at a fixed
interval and attributes CPU utilization to the threads and processes running at the
time of the tick. The following example shows how to use the <tt>tick</tt>
probe to observe this attribution:</p><pre><tt><b># dtrace -n sched:::tick'{@[stringof(args[1]-&gt;pr_fname)] = count()}'</b></tt>
<tt><b>^C</b></tt>
  arch                                                              1
  sh                                                                1
  sed                                                               1
  echo                                                              1
  ls                                                                1
  FvwmAuto                                                          1
  pwd                                                               1
  awk                                                               2
  basename                                                          2
  expr                                                              2
  resize                                                            2
  tput                                                              2
  uname                                                             2
  fsflush                                                           2
  dirname                                                           4
  vim                                                               9
  fvwm2                                                            10
  ksh                                                              19
  xterm                                                            21
  Xsun                                                             93
  MozillaFirebird                                                 260</pre><p>The system clock frequency varies from operating system to operating system, but generally
ranges from 25 hertz to 1024 hertz. The Solaris system clock frequency is
adjustable, but defaults to 100 hertz.</p><p>The <tt>tick</tt> probe only fires if the system clock detects a runnable thread.
To use the <tt>tick</tt> probe to observe the system clock's frequency, you must
have a thread that is always runnable. In one window, create a looping
shell as shown in the following example:</p><pre><tt><b>$ while true ; do let i=0 ; done</b></tt></pre><p>In another window, run the following script:</p><pre>uint64_t last[int];

sched:::tick
/last[cpu]/
{
    @[cpu] = min(timestamp - last[cpu]);
}

sched:::tick
{
    last[cpu] = timestamp;
}</pre><pre><tt><b># dtrace -s ./ticktime.d</b></tt>
dtrace: script './ticktime.d' matched 2 probes
<tt><b>^C</b></tt>

  0          9883789</pre><p>The minimum interval is 9.8 millisecond, which indicates that the default clock tick
frequency is 10 milliseconds (100 hertz). The observed minimum is somewhat less than
10 milliseconds due to jitter.</p><p>One deficiency of tick-based accounting is that the system clock that performs accounting
is often also responsible for dispatching any time-related scheduling activity. As a result,
if a thread is to perform some amount of work every clock tick
(that is, every 10 milliseconds), the system will either over-account for the thread
or under-account for the thread, depending on whether the accounting is done before
or after time-related dispatching scheduling activity. In Solaris, accounting is performed before time-related
dispatching. As a result, the system will under-account for threads running at regular interval.
If such threads run for less than the clock tick interval, they can
effectively &ldquo;hide&rdquo; behind the clock tick. The following example shows the degree to
which the system has such threads:</p><pre>sched:::tick,
sched:::enqueue
{
    @[probename] = lquantize((timestamp / 1000000) % 10, 0, 10);
}</pre><p>The output of the example script is two distributions of the millisecond offset
within a ten millisecond interval, one for the <tt>tick</tt> probe and another for
<tt>enqueue</tt>:</p><pre><tt><b># dtrace -s ./tick.d</b></tt>
dtrace: script './tick.d' matched 4 probes
<tt><b>^C</b></tt>
  tick                                              
           value  -------------- Distribution ------------ count    
               6 |                                         0        
               7 |@                                        3        
               8 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   79       
               9 |                                         0        

  enqueue                                           
           value  -------------- Distribution ------------ count    
             &lt; 0 |                                         0        
               0 |@@                                       267      
               1 |@@                                       300      
               2 |@@                                       259      
               3 |@@                                       291      
               4 |@@@                                      360      
               5 |@@                                       305      
               6 |@@                                       295      
               7 |@@@@                                     522      
               8 |@@@@@@@@@@@@                             1315     
               9 |@@@                                      337</pre><p>The output histogram named <tt>tick</tt> shows that the clock tick is firing at
an 8 millisecond offset. If scheduling were not at all associated with the
clock tick, the output for <tt>enqueue</tt> would be evenly spread across the ten
millisecond interval. However, the output shows a spike at the same 8 millisecond
offset, indicating that at least some threads in the system <b>are</b> being scheduled
on a time basis.</p>
         </div>
      </td>
   </tr>

   <tr class="PageControls" valign="top">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="chp-sched-1.html">Previous</a>
             </td>
             <td align="right">
                 <a href="chp-sched-stability.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
</tbody>
</table>


</body>
</html>

