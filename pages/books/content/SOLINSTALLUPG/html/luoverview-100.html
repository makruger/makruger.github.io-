<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
<title>Solaris Live Upgrade Process - Solaris Express Installation Guide:  Solaris Live Upgrade and Upgrade Planning</title>
<meta name="robots" content="index,follow">
<meta name="robots" content="index,follow">
<meta name="date" content="2007-05-01">
<meta name="collection" content="reference">
<link rel="stylesheet" type="text/css" href="css/elements.css">
<link rel="stylesheet" type="text/css" href="css/indiana.css">
</head>

<body>


<div class="Masthead">
   <div class="MastheadLogo"></div>
   <div class="Title">Solaris Express Installation Guide:  Solaris Live Upgrade and Upgrade Planning</div>
</div>

<table class="Layout" border="0" cellspacing="0" width="100%">
<tbody>

   <tr valign="top" class="PageControls">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="chapter-5.html">Previous</a>
             </td>
             <td align="right">
                 <a href="luplanning-1.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
   
   <tr valign="top">
      <td class="Navigation" width="200px"><p class="toc level1"><a href="docinfo.html">Document Information</a></p>
<p class="toc level1 tocsp"><a href="preface-1.html">Preface</a></p>
<p class="toc level1 tocsp"><a href="esqbt.html">Part&nbsp;I&nbsp;Upgrading With Solaris Live Upgrade</a></p>
<p class="toc level2"><a href="intro-1.html">1.&nbsp;&nbsp;Where to Find Solaris Installation Planning Information</a></p>
<p class="toc level2"><a href="luoverview-1.html">2.&nbsp;&nbsp;Solaris Live Upgrade (Overview)</a></p>
<p class="toc level3"><a href="chapter-5.html">Solaris Live Upgrade Introduction</a></p>
<div class="onpage">
<p class="toc level3"><a href="">Solaris Live Upgrade Process</a></p>
</div>
<p class="toc level2 tocsp"><a href="luplanning-1.html">3.&nbsp;&nbsp;Solaris Live Upgrade (Planning)</a></p>
<p class="toc level2"><a href="lucreate-1.html">4.&nbsp;&nbsp;Using Solaris Live Upgrade to Create a Boot Environment (Tasks)</a></p>
<p class="toc level2"><a href="liveupgrade-1.html">5.&nbsp;&nbsp;Upgrading With Solaris Live Upgrade (Tasks)</a></p>
<p class="toc level2"><a href="luupgrade-85.html">6.&nbsp;&nbsp;Failure Recovery: Falling Back to the Original Boot Environment (Tasks)</a></p>
<p class="toc level2"><a href="lumaintainence-1.html">7.&nbsp;&nbsp;Maintaining Solaris Live Upgrade Boot Environments (Tasks)</a></p>
<p class="toc level2"><a href="grub-101.html">8.&nbsp;&nbsp;x86: Locating the GRUB Menu's <tt>menu.lst</tt> File (Tasks)</a></p>
<p class="toc level2"><a href="gdzlc.html">9.&nbsp;&nbsp;Upgrading the Solaris OS on a System With Non-Global  Zones Installed</a></p>
<p class="toc level2"><a href="luexample-1.html">10.&nbsp;&nbsp;Solaris Live Upgrade (Examples)</a></p>
<p class="toc level2"><a href="lureference-1.html">11.&nbsp;&nbsp;Solaris Live Upgrade (Command Reference)</a></p>
<p class="toc level1 tocsp"><a href="esqcb.html">Part&nbsp;II&nbsp;Appendices</a></p>
<p class="toc level2"><a href="troubleshooting-9.html">A.&nbsp;&nbsp;Troubleshooting (Tasks)</a></p>
<p class="toc level2"><a href="package-9.html">B.&nbsp;&nbsp;Additional SVR4 Packaging Requirements (Reference)</a></p>
<p class="toc level1 tocsp"><a href="glossary-1.html">Glossary</a></p>
<p class="toc level1 tocsp"><a href="idx-1.html">Index</a></p>
</td>
      <td class="ContentPane" width="705px">

	 <div class="MainContent">      	 
             

<a name="luoverview-100"></a><h3>Solaris Live Upgrade Process</h3>
<p><a name="indexterm-2"></a>The following overview describes the tasks necessary to create a copy of the
current boot environment, upgrade the copy, and switch the upgraded copy to become
the active boot environment. The fallback process of switching back to the original
boot environment is also described. <a href="#luoverview-fig-1">Figure&nbsp;2-1</a> describes this complete Solaris Live Upgrade process.</p><a name="luoverview-fig-1"></a><h6>Figure&nbsp;2-1 Solaris Live Upgrade Process</h6><img src="figures/lu-process.gif" alt="The context describes the illustration."></img><p>The following sections describe the Solaris Live Upgrade process.</p>
<ol><li><p>A new boot environment can be created on a physical slice or a logical volume:</p>
<ul><li><p><a href="#overview-3">Creating a Boot Environment</a></p></li>
<li><p><a href="#luoverview-7">Creating a Boot Environment With RAID-1 Volume File Systems</a></p></li></ul>
</li>
<li><p><a href="#luoverview-2">Upgrading a Boot Environment</a></p></li>
<li><p><a href="#luoverview-3">Activating a Boot Environment</a></p></li>
<li><p><a href="#luoverview-4">Falling Back to the Original Boot Environment</a></p></li></ol>


<a name="overview-3"></a><h4>Creating a Boot Environment</h4>
<p><a name="indexterm-3"></a>The process of creating a boot environment provides a method of copying critical
file systems from an active boot environment to a new boot environment. The
disk is reorganized if necessary, file systems are customized, and the critical file
systems are copied to the new boot environment. </p>

<a name="luoverview-13"></a><h5>File System Types</h5>
<p><a name="indexterm-4"></a><a name="indexterm-5"></a><a name="indexterm-6"></a>Solaris Live Upgrade distinguishes between two file system types: critical file systems and
shareable. The following table describes these file system types.</p><table><col width="16.75*"><col width="49.18*"><col width="33.08*"><tr><th align="left" valign="top" scope="column"><p>File System Type</p></th>
<th align="left" valign="top" scope="column"><p>Description </p></th>
<th align="left" valign="top" scope="column"><p>Examples and
More Information</p></th>
</tr>
<tr><td align="left" valign="top" scope="row"><p>Critical file systems</p></td>
<td align="left" valign="top" scope="row"><p>Critical file systems are required by the Solaris OS.
These file systems   are separate mount points in the <tt>vfstab</tt> of
the active and inactive boot environments. These file systems are always copied from the
source to the inactive boot environment. Critical file systems are sometimes referred to
as <b>nonshareable</b>. </p></td>
<td align="left" valign="top" scope="row"><p>Examples are root (<tt>/</tt>), <tt>/usr</tt>, <tt>/var</tt>, or <tt>/opt</tt>. </p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>Shareable file
systems</p></td>
<td align="left" valign="top" scope="row"><p>Shareable file systems are user-defined files such as <tt>/export</tt> that contain the same
mount point in the <tt>vfstab</tt> in both the active and inactive boot environments.
Therefore, updating shared files in the active boot environment also updates data in
the inactive boot environment. When you create a new boot environment, shareable file
systems are shared by default. But you can specify a destination slice and
then the file systems are copied. </p></td>
<td align="left" valign="top" scope="row"><p><tt>/export</tt> is an example of a file
system that can be shared.</p><p>For more detailed information about shareable file systems, see
<a href="luplanning-50.html#luplanning-16">Guidelines for Selecting Slices for Shareable File Systems</a>.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>Swap</p></td>
<td align="left" valign="top" scope="row"><p>Swap is a special shareable file system. Like a shareable file system, all
swap slices are shared by default. But, if you specify a destination
directory for swap, the swap slice is copied.</p></td>
<td align="left" valign="top" scope="row"><p> For procedures about reconfiguring swap,
see <a href="chapter-33.html#lucreate-13">To Create a Boot Environment and Reconfiguring Swap</a>.</p></td>
</tr>
</table>

<a name="esqbv"></a><h5>Creating RAID-1 Volumes on File Systems</h5>
<p>Solaris Live Upgrade can create a boot environment with RAID-1 volumes (mirrors) on
file systems. For an overview, see <a href="#luoverview-7">Creating a Boot Environment With RAID-1 Volume File Systems</a>.</p>

<a name="espyv"></a><h5>Copying File Systems</h5>
<p>The process of creating a new boot environment begins by identifying an unused
slice where a critical file system can be copied. If a slice
is not available or a slice does not meet the minimum requirements, you
need to format a new slice. </p><p>After the slice is defined, you can reconfigure the file systems on the
new boot environment before the file systems are copied into the directories. You
reconfigure file systems by splitting and merging them, which provides a simple way
of editing the <tt>vfstab</tt> to connect and disconnect file system directories. You can
merge file systems into their parent directories by specifying the same mount point.
You can also split file systems from their parent directories by specifying different
mount points. </p><p>After file systems are configured on the inactive boot environment, you begin the
automatic copy. Critical file systems are copied to the designated directories. Shareable file
systems are not copied, but are shared. The exception is that you can
 designate some shareable file systems to be copied.  When the file
systems are copied from the active to the inactive boot environment, the files
are directed to the new directories. The active boot environment is not changed
in any way. </p><table><col width="50*"><col width="50*"><tr><td align="left" valign="top" scope="row"><p>For procedures to split or merging file systems</p></td>
<td align="left" valign="top" scope="row">
<ul><li><p><a href="chapter-33.html#lucreate-15">To Create a Boot Environment and Merge File Systems </a></p></li>
<li><p><a href="chapter-33.html#lucreate-14">To Create a Boot Environment and Split File Systems</a></p></li></ul>
</td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>For an
overview of creating a boot environment with RAID&ndash;1 volume file systems</p></td>
<td align="left" valign="top" scope="row"><p><a href="#luoverview-7">Creating a Boot Environment With RAID-1 Volume File Systems</a></p></td>
</tr>
</table>

<a name="esqcg"></a><h5>Examples of Creating a New Boot Environment</h5>
<p>The following figures illustrate various ways of creating new boot environments.</p><p><a href="#luoverview-fig-25">Figure&nbsp;2-2</a> shows that critical file system root (<tt>/</tt>) has been copied to another slice
on a disk to create a new boot environment. The  active
boot environment contains the root (<tt>/</tt>) file system on one slice. The new boot
environment is an exact duplicate with the root (<tt>/</tt>) file system on a
new slice. The file systems <tt>/swap</tt> and <tt>/export/home</tt> are shared by the
active and inactive boot environments.</p><a name="luoverview-fig-25"></a><h6>Figure&nbsp;2-2 Creating an Inactive Boot Environment &ndash; Copying the root (<tt>/</tt>) File System</h6><img src="figures/lu-create-root.gif" alt="The context describes the illustration."></img><p><a href="#luoverview-fig-3">Figure&nbsp;2-3</a> shows critical file systems that have been split and have been copied
to slices on a disk to create a new boot environment. The 
active boot environment contains the root (<tt>/</tt>) file system on one slice. On
that slice, the root (<tt>/</tt>) file system contains the <tt>/usr</tt>, <tt>/var</tt>, and <tt>/opt</tt>
directories. In the new boot environment, the root (<tt>/</tt>) file system is split
and <tt>/usr</tt> and <tt>/opt</tt> are put on separate slices. The file systems <tt>/swap</tt>
and <tt>/export/home</tt> are shared by both boot environments.</p><a name="luoverview-fig-3"></a><h6>Figure&nbsp;2-3 Creating an Inactive Boot Environment &ndash; Splitting File Systems</h6><img src="figures/lu-create-split.gif" alt="The context describes the illustration."></img><p><a href="#luoverview-fig-26">Figure&nbsp;2-4</a> shows critical file systems that have been merged and have been copied
to slices on a disk to create a new boot environment. The 
active boot environment contains the root (<tt>/</tt>) file system, <tt>/usr</tt>, <tt>/var</tt>, and <tt>/opt</tt>
with each file system on their own slice. In the new boot environment,
/usr and <tt>/opt</tt> are merged into the root (<tt>/</tt>) file system on one
slice. The file systems <tt>/swap</tt> and <tt>/export/home</tt> are shared by both boot environments.</p><a name="luoverview-fig-26"></a><h6>Figure&nbsp;2-4 Creating an Inactive Boot Environment &ndash; Merging File Systems</h6><img src="figures/lu-create-merge.gif" alt="The context describes the illustration."></img>

<a name="luoverview-7"></a><h4>Creating a Boot Environment With RAID-1 Volume File Systems</h4>
<p><a name="indexterm-7"></a><a name="indexterm-8"></a><a name="indexterm-9"></a>Solaris Live Upgrade uses Solaris Volume Manager technology to create a boot environment
that can contain file systems encapsulated in RAID-1 volumes. Solaris Volume Manager provides
a powerful way to reliably manage your disks and data by using volumes.
Solaris Volume Manager enables concatenations, stripes, and other complex configurations. Solaris Live Upgrade enables
a subset of these tasks, such as creating a RAID-1 volume for
the root (<tt>/</tt>) file system. </p><p>A volume can group disk slices across several disks to transparently appear as
a single disk to the OS. Solaris Live Upgrade is limited to
creating a boot environment for the root (<tt>/</tt>) file system that contains single-slice concatenations
inside a RAID-1 volume (mirror). This limitation is because the boot PROM is
restricted to choosing one slice from which to boot. </p>

<a name="esqcc"></a><h5>How to Manage Volumes With Solaris Live Upgrade</h5>
<p>When creating a boot environment, you can use Solaris Live Upgrade to manage
the following tasks.</p>
<ul><li><p>Detach a single-slice concatenation (submirror) from a RAID-1 volume (mirror). The contents can be preserved to become the content of the new boot environment if necessary. Because the contents are not copied, the new boot environment can be quickly created. After the submirror is detached from the original mirror, the submirror is no longer part of the mirror. Reads and writes on the submirror are no longer performed through the mirror.</p></li>
<li><p>Create a boot environment that contains a mirror.</p></li>
<li><p>Attach a maximum of three single-slice concatenations to the newly created mirror.</p></li></ul>
<p>You use the <tt>lucreate</tt> command with the <tt>-m</tt> option to create a mirror,
detach submirrors, and attach submirrors for the new boot environment. </p>
<hr><p><b>Note - </b>If VxVM volumes are configured on your current system, the <tt>lucreate</tt> command
can create a new boot environment.  When the data is copied to
the new boot environment, the Veritas file system configuration is lost and a
UFS file system is created on the new boot environment.</p>
<hr>
<table><col width="50*"><col width="50*"><tr><td align="left" valign="top" scope="row"><p>For step-by-step procedures</p></td>
<td align="left" valign="top" scope="row"><p><a href="chapter-33.html#lucreate-1000">To Create a Boot Environment With RAID-1 Volumes (Mirrors)</a></p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>For
an overview of creating RAID-1 volumes when installing</p></td>
<td align="left" valign="top" scope="row"><p><a href="http://docs.sun.com/doc/820-0157/mirroroverview-1?a=view">Chapter 8, Creating RAID-1 Volumes (Mirrors) During Installation (Overview), in <i>Solaris Express Installation Guide: Planning for Installation and Upgrade</i></a></p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>For in-depth information about other
complex Solaris Volume Manager configurations that are not supported if you are using
Solaris Live Upgrade</p></td>
<td align="left" valign="top" scope="row"><p><a href="http://docs.sun.com/doc/819-2789/storage-overview-1?a=view">Chapter 2, Storage Management Concepts, in <i>Solaris Volume Manager Administration Guide</i></a></p></td>
</tr>
</table>

<a name="esqce"></a><h5>Mapping Solaris Volume Manager Tasks to Solaris Live Upgrade</h5>
<p>Solaris Live Upgrade manages a subset of Solaris Volume Manager tasks. <a href="#luoverview-tbl-900">Table&nbsp;2-1</a>
shows the Solaris Volume Manager components that Solaris Live Upgrade can manage.</p><a name="luoverview-tbl-900"></a><h6>Table&nbsp;2-1 Classes of Volumes</h6><table><col width="22.94*"><col width="77.06*"><tr><th align="left" valign="top" scope="column"><p>Term</p></th>
<th align="left" valign="top" scope="column"><p>Description</p></th>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-10"></a>concatenation</p></td>
<td align="left" valign="top" scope="row"><p>A RAID-0
volume. If slices are concatenated, the data is written to the first available
slice until that slice is full. When that slice is full, the data
is written to the next slice, serially. A concatenation provides no data redundancy
unless it is contained in a mirror.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-11"></a>mirror</p></td>
<td align="left" valign="top" scope="row"><p>A RAID-1 volume. See RAID-1 volume.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-12"></a>RAID-1
volume</p></td>
<td align="left" valign="top" scope="row"><p>A class of volume that replicates data by maintaining multiple copies. A RAID-1
volume is sometimes called a mirror. A RAID-1 volume is composed of one
or more RAID-0 volumes that  are called submirrors. </p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-13"></a><a name="indexterm-14"></a>RAID-0 volume</p></td>
<td align="left" valign="top" scope="row"><p>A class of
volume that can be a stripe or a concatenation. These components are also
called submirrors. A stripe or concatenation is the basic building block for mirrors.
</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-15"></a>state database</p></td>
<td align="left" valign="top" scope="row"><p>A state database stores information about disk about the state of
your Solaris Volume Manager configuration. The state database is a collection of multiple, replicated
database copies. Each copy is referred to as a state database replica. The
state database tracks the location and status of all known state database replicas.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>state
database replica</p></td>
<td align="left" valign="top" scope="row"><p>A copy of a state database. The replica ensures that the
data in the database is valid.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-16"></a>submirror</p></td>
<td align="left" valign="top" scope="row"><p>See RAID-0 volume.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p><a name="indexterm-17"></a><a name="indexterm-18"></a>volume</p></td>
<td align="left" valign="top" scope="row"><p>A group of physical slices or
other volumes that appear to the system as a single logical device. A
volume is functionally identical to a physical disk in the view of an
application or file system.  In some command-line utilities, a volume is called
a metadevice.  </p></td>
</tr>
</table>

<a name="esqby"></a><h5>Examples of Using Solaris Live Upgrade to Create RAID-1 Volumes</h5>
<p>The following examples present command syntax for creating RAID-1 volumes for a new
boot environment.</p>

<a name="esqcd"></a><h5>Create RAID-1 Volume on Two Physical Disks</h5>
<p><a href="#luoverview-fig-10">Figure&nbsp;2-5</a> shows a new boot environment with a RAID-1 volume (mirror) that is
created on two physical disks. The following command created the new boot environment
and the mirror.</p><pre># <tt><b>lucreate -n second_disk -m /:/dev/md/dsk/d30:mirror,ufs \ -m /:/dev/dsk/c0t1d0s0,/dev/md/dsk/d31:attach -m /:/dev/dsk/c0t2d0s0,/dev/md/dsk/d32:attach \ -m -:/dev/dsk/c0t1d0s1:swap -m -:/dev/dsk/c0t2d0s1:swap</b></tt></pre><p>This command performs the following tasks: </p>
<ul><li><p>Creates a new boot environment, <tt>second_disk</tt>. </p></li>
<li><p>Creates a mirror <tt>d30</tt> and configures a UFS file system.</p></li>
<li><p>Creates a single-device concatenation on slice 0 of each physical disk. The concatenations are named <tt>d31</tt> and <tt>d32</tt>.</p></li>
<li><p>Adds the two concatenations to mirror <tt>d30</tt>.</p></li>
<li><p>Copies the root (<tt>/</tt>) file system to the mirror.</p></li>
<li><p>Configures files systems for swap on slice 1 of each physical disk.</p></li></ul>
<a name="luoverview-fig-10"></a><h6>Figure&nbsp;2-5 Create a Boot Environment and Create a Mirror</h6><img src="figures/lu-create-mirr.gif" alt="The context describes the illustration."></img>

<a name="esqci"></a><h5>Create a Boot Environment and Use the Existing Submirror</h5>
<p><a href="#luoverview-fig-11">Figure&nbsp;2-6</a> shows a new boot environment that contains a RAID-1 volume (mirror). The
following command created the new boot environment and the mirror.</p><pre># <tt><b>lucreate -n second_disk -m /:/dev/md/dsk/d20:ufs,mirror \ -m /:/dev/dsk/c0t1d0s0:detach,attach,preserve</b></tt></pre><p>This command performs the following tasks: </p>
<ul><li><p>Creates a new boot environment, <tt>second_disk</tt>.</p></li>
<li><p>Breaks mirror <tt>d10</tt> and detaches concatenation <tt>d12</tt>.</p></li>
<li><p>Preserves the contents of concatenation <tt>d12</tt>. File systems are not copied.</p></li>
<li><p>Creates a new mirror <tt>d20</tt>. You now have two one-way mirrors <tt>d10</tt> and <tt>d20</tt>.</p></li>
<li><p>Attaches concatenation <tt>d12</tt> to mirror <tt>d20</tt>.</p></li></ul>
<a name="luoverview-fig-11"></a><h6>Figure&nbsp;2-6 Create a Boot Environment and Use the Existing Submirror</h6><img src="figures/lu-create-det-mirr.gif" alt="The illustration provides the context."></img>

<a name="luoverview-2"></a><h4>Upgrading a Boot Environment</h4>
<p><a name="indexterm-19"></a>After you have created a boot environment, you can perform an upgrade on
the boot environment. As part of that upgrade, the boot environment can contain
RAID-1 volumes (mirrors) for any file systems. Or the boot environment can have
non-global zones installed. The upgrade does not affect any files in the active
boot environment. When you are ready, you activate the new boot environment, which
then becomes the current boot environment. </p><table><col width="50*"><col width="50*"><tr><td align="left" valign="top" scope="row"><p>For procedures about upgrading a boot
environment</p></td>
<td align="left" valign="top" scope="row"><p><a href="liveupgrade-1.html">Chapter&nbsp;5, Upgrading With Solaris Live Upgrade (Tasks)</a></p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>For an example of upgrading a boot environment with a RAID&ndash;1 volume file
system</p></td>
<td align="left" valign="top" scope="row"><p><a href="luexample-90.html">Example of Detaching and Upgrading One Side of a RAID-1 Volume (Mirror)</a></p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>For procedures about upgrading with non-global zones</p></td>
<td align="left" valign="top" scope="row"><p><a href="gdzlc.html">Chapter&nbsp;9, Upgrading the Solaris OS on a System With Non-Global  Zones Installed</a></p></td>
</tr>
</table><p><a href="#luoverview-fig-5">Figure&nbsp;2-7</a> shows an upgrade to an inactive boot environment. </p><a name="luoverview-fig-5"></a><h6>Figure&nbsp;2-7 Upgrading an Inactive Boot Environment</h6><img src="figures/lu-upgrd-boot.gif" alt="The context describes the illustration."></img><p>Rather than an upgrade, you can install a Solaris Flash archive on a
boot environment. The Solaris Flash installation feature enables you to create a single
reference installation of the Solaris OS on a system. This system is called
the master system. Then, you can replicate that installation on a number of
systems that are called clone systems. In this situation, the inactive boot environment
is a clone. When you install the Solaris Flash archive on a system,
the archive replaces all the files on the existing boot environment as an
initial installation would. </p><p>For procedures about installing a Solaris Flash archive, see <a href="luupgrade-83.html">Installing Solaris Flash Archives on a Boot Environment</a>.</p><p><a name="indexterm-20"></a>The following figures show an installation of a Solaris Flash archive on an
inactive boot environment. <a href="#luoverview-fig-12">Figure&nbsp;2-8</a> shows a system with a single hard disk.
<a href="#luoverview-fig-12b">Figure&nbsp;2-9</a> shows a system with two hard disks.</p><a name="luoverview-fig-12"></a><h6>Figure&nbsp;2-8 Installing a Solaris Flash Archive on a Single Disk</h6><img src="figures/lu-flash-boot.gif" alt="The context describes the illustration."></img><a name="luoverview-fig-12b"></a><h6>Figure&nbsp;2-9 Installing a Solaris Flash Archive on Two Disks</h6><img src="figures/lu-flash-boot2.gif" alt="The context describes the illustration."></img>

<a name="luoverview-3"></a><h4>Activating a Boot Environment</h4>
<p><a name="indexterm-21"></a>When you are ready to switch and make the new boot environment active,
you quickly activate the new boot environment and reboot. Files are synchronized between
boot environments the first time that you boot a newly created boot environment.
&ldquo;Synchronize&rdquo; means that certain system files and directories are copied from the last-active
boot environment to the boot environment being booted. When you reboot the system,
the configuration that you installed on the new boot environment is active. The
original boot environment then becomes an inactive boot environment. </p><table><col width="50*"><col width="50*"><tr><td align="left" valign="top" scope="row"><p>For procedures about activating
a boot environment</p></td>
<td align="left" valign="top" scope="row"><p><a href="luupgrade-84.html">Activating a Boot Environment</a></p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>For information about synchronizing the active and inactive boot environment</p></td>
<td align="left" valign="top" scope="row"><p><a href="luplanning-10.html">Synchronizing Files Between Boot Environments</a></p></td>
</tr>
</table><p><a href="#chapter-fig-13">Figure&nbsp;2-10</a> shows a switch after a reboot from an inactive to an active
boot environment.</p><a name="chapter-fig-13"></a><h6>Figure&nbsp;2-10 Activating an Inactive Boot Environment</h6><img src="figures/lu-act-boot.gif" alt="The context describes the illustration."></img>

<a name="luoverview-4"></a><h4>Falling Back to the Original Boot Environment</h4>
<p><a name="indexterm-22"></a><a name="indexterm-23"></a>If a failure occurs, you can quickly fall back to the original boot
environment with an activation and reboot. The use of fallback takes only the
time to reboot the system, which is much quicker than backing up and
restoring the original. The new boot environment that failed to boot is preserved.
The failure can then  be analyzed. You can only fall back to
the boot environment that was used by <tt>luactivate</tt> to activate the new boot
environment. </p><p>You fall back to the previous boot environment the following ways:</p><table><col width="33.06*"><col width="66.94*"><tr><th align="left" valign="top" scope="column"><p>Problem</p></th>
<th align="left" valign="top" scope="column"><p>Action</p></th>
</tr>
<tr><td align="left" valign="top" scope="row"><p>The new
boot environment boots successfully, but you are not happy with the results.</p></td>
<td align="left" valign="top" scope="row"><p>Run the
<tt>luactivate</tt> command with the name of the previous boot environment and reboot.</p>
<hr><p><b>x86 only - </b><b>Starting with the Solaris 10 1/06 release</b>, you
can fall back by selecting the original boot environment that is found on
the GRUB menu. The original boot environment and the new boot environment must
be based on the GRUB software. Booting from the GRUB menu does not
synchronize files between the old and new boot environments. For more information about
synchronizing files, see <a href="luplanning-10.html#luplanning-1110">Forcing a Synchronization Between Boot Environments</a>.</p>
<hr>
</td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>The new boot environment does not boot.</p></td>
<td align="left" valign="top" scope="row"><p>Boot the fallback
boot environment in single-user mode, run the <tt>luactivate</tt> command, and reboot.</p></td>
</tr>
<tr><td align="left" valign="top" scope="row"><p>You cannot
boot in single-user mode.</p></td>
<td align="left" valign="top" scope="row"><p>Perform one of the following:</p>
<ul><li><p>Boot from DVD or CD media or a net installation image</p></li>
<li><p>Mount the root (<tt>/</tt>) file system on the fallback boot environment</p></li>
<li><p>Run the <tt>luactivate</tt> command and reboot</p></li></ul>
</td>
</tr>
</table><p>For procedures to fall back, see <a href="luupgrade-85.html">Chapter&nbsp;6, Failure Recovery: Falling Back to the Original Boot Environment (Tasks)</a>.</p><p><a href="#chapter-fig-74">Figure&nbsp;2-11</a> shows the switch that is made when you reboot to fallback. </p><a name="chapter-fig-74"></a><h6>Figure&nbsp;2-11 Fallback to the Original Boot Environment</h6><img src="figures/lu-fallback.gif" alt="The context describes the illustration."></img>

<a name="luoverview-5"></a><h4>Maintaining a Boot Environment</h4>
<p>You can also do various maintenance activities such as checking status, renaming, or
deleting a boot environment. For maintenance procedures, see <a href="lumaintainence-1.html">Chapter&nbsp;7, Maintaining Solaris Live Upgrade Boot Environments (Tasks)</a>.</p>
         </div>
      </td>
   </tr>

   <tr class="PageControls" valign="top">
      <td></td>
      <td>
         <table width="100%">
      	   <tr>
      	     <td>
                 <a href="chapter-5.html">Previous</a>
             </td>
             <td align="right">
                 <a href="luplanning-1.html">Next</a>
             </td>
           </tr>
         </table>
      </td>
   </tr>
</tbody>
</table>


</body>
</html>

